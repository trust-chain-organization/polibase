"""åŒæœŸçš„ãªè­°äº‹éŒ²å‡¦ç†å®Ÿè¡Œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""

import os
from dataclasses import dataclass
from datetime import datetime
from typing import Any

from src.common.logging import get_logger
from src.domain.entities.conversation import Conversation
from src.domain.entities.minutes import Minutes
from src.domain.entities.speaker import Speaker
from src.exceptions import APIKeyError, ProcessingError
from src.infrastructure.external.llm_service_factory import LLMServiceFactory
from src.infrastructure.persistence.conversation_repository_impl import (
    ConversationRepositoryImpl as AsyncConversationRepo,
)
from src.infrastructure.persistence.meeting_repository_impl import (
    MeetingRepositoryImpl as AsyncMeetingRepo,
)
from src.infrastructure.persistence.minutes_repository_impl import (
    MinutesRepositoryImpl as AsyncMinutesRepo,
)
from src.infrastructure.persistence.repository_adapter import RepositoryAdapter
from src.infrastructure.persistence.speaker_repository_impl import (
    SpeakerRepositoryImpl as AsyncSpeakerRepo,
)
from src.minutes_divide_processor.minutes_process_agent import MinutesProcessAgent
from src.streamlit.utils.processing_logger import ProcessingLogger
from src.utils.gcs_storage import GCSStorage

logger = get_logger(__name__)


@dataclass
class SyncMinutesProcessingResult:
    """åŒæœŸå‡¦ç†çµæœ"""

    minutes_id: int
    meeting_id: int
    total_conversations: int
    unique_speakers: int
    processing_time_seconds: float
    processed_at: datetime
    errors: list[str] | None = None


class SyncMinutesProcessor:
    """åŒæœŸçš„ã«è­°äº‹éŒ²å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, meeting_id: int):
        """åˆæœŸåŒ–

        Args:
            meeting_id: å‡¦ç†å¯¾è±¡ã®ä¼šè­°ID
        """
        self.meeting_id = meeting_id
        self.logger = ProcessingLogger()

    def process(self) -> SyncMinutesProcessingResult:
        """è­°äº‹éŒ²å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹

        Returns:
            å‡¦ç†çµæœ
        """
        start_time = datetime.now()
        errors = []

        try:
            self.logger.add_log(self.meeting_id, "å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™", "info")
            self.logger.add_log(
                self.meeting_id,
                f"ä¼šè­°ID {self.meeting_id} ã®è­°äº‹éŒ²å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™",
                "info",
            )

            # åŒæœŸãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨
            self.logger.add_log(
                self.meeting_id, "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã—ã¦ã„ã¾ã™...", "info"
            )
            meeting_repo = RepositoryAdapter(AsyncMeetingRepo)
            minutes_repo = RepositoryAdapter(AsyncMinutesRepo)
            conversation_repo = RepositoryAdapter(AsyncConversationRepo)
            speaker_repo = RepositoryAdapter(AsyncSpeakerRepo)

            self.logger.add_log(self.meeting_id, "ä¼šè­°æƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™...", "info")
            # ä¼šè­°æƒ…å ±ã‚’å–å¾—
            meeting = meeting_repo.get_by_id(self.meeting_id)
            if not meeting:
                raise ValueError(f"Meeting {self.meeting_id} not found")

            # æ—¢å­˜ã®è­°äº‹éŒ²ã‚’ãƒã‚§ãƒƒã‚¯
            existing_minutes = minutes_repo.get_by_meeting(self.meeting_id)

            # æ—¢å­˜ã®Conversationsã‚’ãƒã‚§ãƒƒã‚¯
            if existing_minutes and existing_minutes.id:
                conversations = conversation_repo.get_by_minutes(existing_minutes.id)
                if conversations:
                    raise ValueError(
                        f"Meeting {self.meeting_id} already has conversations"
                    )

            # è­°äº‹éŒ²ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            self.logger.add_log(
                self.meeting_id, "è­°äº‹éŒ²ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¦ã„ã¾ã™...", "info"
            )
            extracted_text = self._fetch_minutes_text(meeting)

            # å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®æ¦‚è¦ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            text_length = len(extracted_text)
            preview_length = min(500, text_length)  # æœ€åˆã®500æ–‡å­—ã¾ã§è¡¨ç¤º
            text_preview = extracted_text[:preview_length]
            if text_length > preview_length:
                text_preview += f"\n\n... (å…¨{text_length:,}æ–‡å­—)"

            self.logger.add_log(
                self.meeting_id,
                f"ğŸ“„ è­°äº‹éŒ²ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆ{text_length:,}æ–‡å­—ï¼‰",
                "info",
                details=text_preview,
            )

            # Minutes ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã¾ãŸã¯å–å¾—
            if not existing_minutes:
                minutes = Minutes(
                    meeting_id=self.meeting_id,
                    url=meeting.url,
                )
                minutes = minutes_repo.create(minutes)
            else:
                minutes = existing_minutes

            # è­°äº‹éŒ²ã‚’å‡¦ç†
            self.logger.add_log(self.meeting_id, "è­°äº‹éŒ²ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™...", "info")
            results = self._process_minutes(extracted_text)

            # æŠ½å‡ºçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            if results:
                result_summary = []
                for i, result in enumerate(results[:5], 1):  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                    speaker = getattr(result, "speaker", "ä¸æ˜")
                    content = getattr(result, "speech_content", "")
                    preview = content[:100] + "..." if len(content) > 100 else content
                    result_summary.append(f"{i}. {speaker}: {preview}")

                if len(results) > 5:
                    result_summary.append(f"\n... ä»–{len(results) - 5}ä»¶ã®ç™ºè¨€")

                self.logger.add_log(
                    self.meeting_id,
                    f"ğŸ“ {len(results)}ä»¶ã®ç™ºè¨€ã‚’æŠ½å‡ºã—ã¾ã—ãŸ",
                    "info",
                    details="\n".join(result_summary),
                )

            # Conversationsã‚’ä¿å­˜
            self.logger.add_log(self.meeting_id, "ç™ºè¨€ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™...", "info")
            saved_conversations = self._save_conversations(
                results, minutes.id, conversation_repo
            )

            # Speakersã‚’æŠ½å‡ºãƒ»ä½œæˆ
            self.logger.add_log(self.meeting_id, "ç™ºè¨€è€…ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...", "info")
            unique_speakers = self._extract_and_create_speakers(
                saved_conversations, speaker_repo
            )

            # å‡¦ç†å®Œäº†æ™‚é–“ã‚’è¨ˆç®—
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            self.logger.add_log(self.meeting_id, "âœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ", "success")
            self.logger.add_log(
                self.meeting_id,
                f"æŠ½å‡ºã•ã‚ŒãŸç™ºè¨€æ•°: {len(saved_conversations)}ä»¶",
                "info",
            )
            self.logger.add_log(
                self.meeting_id, f"æŠ½å‡ºã•ã‚ŒãŸç™ºè¨€è€…æ•°: {unique_speakers}äºº", "info"
            )
            self.logger.add_log(
                self.meeting_id, f"å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’", "info"
            )

            # ãƒªãƒã‚¸ãƒˆãƒªã‚’é–‰ã˜ã‚‹
            meeting_repo.close()
            minutes_repo.close()
            conversation_repo.close()
            speaker_repo.close()

            return SyncMinutesProcessingResult(
                minutes_id=minutes.id if minutes.id else 0,
                meeting_id=self.meeting_id,
                total_conversations=len(saved_conversations),
                unique_speakers=unique_speakers,
                processing_time_seconds=processing_time,
                processed_at=end_time,
                errors=errors if errors else None,
            )

        except Exception as e:
            self.logger.add_log(
                self.meeting_id, f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", "error"
            )
            logger.error(
                f"Processing failed for meeting {self.meeting_id}: {e}", exc_info=True
            )
            raise

    def _fetch_minutes_text(self, meeting: Any) -> str:
        """è­°äº‹éŒ²ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
        if meeting.gcs_text_uri:
            try:
                from src.config import config

                gcs_storage = GCSStorage(
                    bucket_name=config.GCS_BUCKET_NAME,
                    project_id=config.GCS_PROJECT_ID,
                )
                text = gcs_storage.download_content(meeting.gcs_text_uri)
                if text:
                    logger.info(
                        f"Downloaded text from GCS ({len(text)} characters)",
                        meeting_id=meeting.id,
                    )
                    return text
            except Exception as e:
                logger.warning(f"Failed to download from GCS: {e}")

        if meeting.gcs_pdf_uri:
            raise ValueError(
                f"PDF processing not yet implemented for meeting {meeting.id}"
            )

        raise ValueError(f"No valid source found for meeting {meeting.id}")

    def _process_minutes(self, text: str) -> list[Any]:
        """è­°äº‹éŒ²ã‚’å‡¦ç†ã—ã¦ç™ºè¨€ã‚’æŠ½å‡ºã™ã‚‹"""
        if not text:
            raise ProcessingError("No text provided for processing", {"text_length": 0})

        # APIã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        if not os.getenv("GOOGLE_API_KEY"):
            raise APIKeyError(
                "GOOGLE_API_KEY not set. Please configure it in your .env file",
                {"env_var": "GOOGLE_API_KEY"},
            )

        # LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½œæˆ
        llm_service = LLMServiceFactory.create_gemini_service(temperature=0.0)

        # MinutesProcessAgentã‚’ä½¿ç”¨ã—ã¦å‡¦ç†
        agent = MinutesProcessAgent(llm_service=llm_service)

        logger.info(f"Processing minutes (text length: {len(text)})")

        # åŒæœŸçš„ã«å®Ÿè¡Œ
        results = agent.run(text)

        logger.info(f"Extracted {len(results)} conversations")
        return results

    def _save_conversations(
        self, results: list[Any], minutes_id: int, repo: Any
    ) -> list[Conversation]:
        """ç™ºè¨€ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹"""
        conversations = []
        for idx, result in enumerate(results):
            conv = Conversation(
                minutes_id=minutes_id,
                speaker_name=result.speaker,
                comment=result.speech_content,
                sequence_number=idx + 1,
            )
            conversations.append(conv)

        # ãƒãƒ«ã‚¯ä½œæˆ
        saved = repo.bulk_create(conversations)
        logger.info(
            f"Saved {len(saved)} conversations to database", minutes_id=minutes_id
        )
        return saved

    def _extract_and_create_speakers(
        self, conversations: list[Conversation], speaker_repo: Any
    ) -> int:
        """ç™ºè¨€ã‹ã‚‰ä¸€æ„ãªç™ºè¨€è€…ã‚’æŠ½å‡ºã—ã€ç™ºè¨€è€…ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹"""
        from src.domain.services.speaker_domain_service import SpeakerDomainService

        speaker_service = SpeakerDomainService()
        speaker_names = set()

        for conv in conversations:
            if conv.speaker_name:
                clean_name, party_info = speaker_service.extract_party_from_name(
                    conv.speaker_name
                )
                speaker_names.add((clean_name, party_info))

        # ç™ºè¨€è€…ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
        created_count = 0
        for name, party_info in speaker_names:
            # æ—¢å­˜ã®ç™ºè¨€è€…ã‚’ãƒã‚§ãƒƒã‚¯
            existing = speaker_repo.get_by_name_party_position(name, party_info, None)

            if not existing:
                # æ–°è¦ç™ºè¨€è€…ã‚’ä½œæˆ
                speaker = Speaker(
                    name=name,
                    political_party_name=party_info,
                    is_politician=bool(party_info),
                )
                speaker_repo.create(speaker)
                created_count += 1

        logger.info(f"Created {created_count} new speakers")
        return created_count
